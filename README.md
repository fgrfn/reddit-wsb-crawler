<div align="center">

<img src="logo.png" alt="WSB-Crawler Logo" width="200"/>

# üöÄ WSB-Crawler

[![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)](https://github.com/fgrfn/reddit-wsb-crawler/releases)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://github.com/fgrfn/reddit-wsb-crawler/pkgs/container/reddit-wsb-crawler)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-2088FF.svg)](https://github.com/fgrfn/reddit-wsb-crawler/actions)

**Automatisches Fr√ºhwarnsystem f√ºr Reddit-Aktien-Hypes**

Crawlt r/wallstreetbets nach Ticker-Erw√§hnungen, analysiert Trends und sendet Discord-Alerts bei ungew√∂hnlicher Aktivit√§t.

[Features](#-features) ‚Ä¢ [Quick Start](#-quick-start) ‚Ä¢ [Docker](#-docker-empfohlen) ‚Ä¢ [Konfiguration](#-konfiguration) ‚Ä¢ [Dokumentation](#-dokumentation)

</div>

---

## ‚ú® Features

### üîç **Intelligentes Crawling**
- Durchsucht konfigurierbare Subreddits (wallstreetbets, wallstreetbetsGER, etc.)
- Regex-basierte Ticker-Erkennung in Posts & Kommentaren
- Parallel-Processing f√ºr schnelle Analyse
- Deduplizierung und Blacklist-Filter

### üìä **Umfassende Datenanalyse**
- **Kursdaten**: Live-Kurse + Pre/After-Market von Yahoo Finance
- **Trend-Analyse**: 1h, 24h, 7d Kursver√§nderungen
- **News-Integration**: Aktuelle Headlines via NewsAPI
- **Historischer Vergleich**: Erkennt signifikante Anstiege

### üîî **Smart Alerts**
- Discord-Benachrichtigungen bei ungew√∂hnlicher Aktivit√§t
- Konfigurierbare Schwellwerte (Nennungen, Kurs√§nderungen)
- Kompaktes Format mit allen relevanten Infos
- Silent Status-Updates (Heartbeat ohne Ping)

### üê≥ **Production-Ready**
- Vollst√§ndige Docker-Unterst√ºtzung
- Automatische Releases via GitHub Actions
- Semantic Versioning mit Auto-Increment
- Persistente Daten & Caching

## üöÄ Quick Start

### Option 1: Docker (empfohlen)

```bash
# Pre-built Image von GitHub Container Registry
docker pull ghcr.io/fgrfn/reddit-wsb-crawler:latest

# Oder mit spezifischer Version
docker pull ghcr.io/fgrfn/reddit-wsb-crawler:v1.0.1
```

**Alternativ: Selbst bauen**

```bash
# 1. Repository klonen
git clone https://github.com/fgrfn/reddit-wsb-crawler.git
cd reddit-wsb-crawler

# 2. Config erstellen
cp config/.env.example config/.env
nano config/.env  # API-Keys eintragen

# 3. Mit Docker Compose starten
docker-compose up -d

# 4. Logs anschauen
docker-compose logs -f
```

### Option 2: Python (lokal)
### Option 2: Python (lokal)

```bash
# 1. Repository klonen
git clone https://github.com/fgrfn/reddit-wsb-crawler.git
cd reddit-wsb-crawler

# 2. Virtual Environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Dependencies installieren
pip install -r requirements.txt

# 4. Config erstellen
cp config/.env.example config/.env
nano config/.env  # API-Keys eintragen

# 5. Crawler starten
python src/run_crawler_headless.py
```

---

## ‚öôÔ∏è Konfiguration

#### Erforderliche Credentials:

| Variable | Beschreibung | Wo bekomme ich das? |
|----------|--------------|---------------------|
| `REDDIT_CLIENT_ID` | Reddit API Client ID | [reddit.com/prefs/apps](https://www.reddit.com/prefs/apps) |
| `REDDIT_CLIENT_SECRET` | Reddit API Secret | [reddit.com/prefs/apps](https://www.reddit.com/prefs/apps) |
| `REDDIT_USER_AGENT` | User Agent String | z.B. `python:wsb-crawler:v1.0.0 (by /u/your_username)` |
| `NEWSAPI_KEY` | NewsAPI Key | [newsapi.org/register](https://newsapi.org/register) |
| `DISCORD_WEBHOOK_URL` | Discord Webhook URL | Discord Server Settings ‚Üí Integrations ‚Üí Webhooks |

#### Optionale Einstellungen:

```ini
# Subreddits (komma-separiert)
SUBREDDITS=wallstreetbets,wallstreetbetsGER,mauerstrassenwetten

# News-Einstellungen
NEWSAPI_LANG=en           # Sprache f√ºr News (en, de, etc.)
NEWSAPI_WINDOW_HOURS=48   # Zeitfenster f√ºr News in Stunden

---

## üìÇ Projektstruktur

```
reddit-wsb-crawler/
‚îú‚îÄ‚îÄ src/                              # Quellcode
‚îÇ   ‚îú‚îÄ‚îÄ run_crawler_headless.py      # Haupt-Entry-Point (vollautomatisch)
‚îÇ   ‚îú‚îÄ‚îÄ main_crawler.py              # Legacy-Version (einfacher)
‚îÇ   ‚îú‚îÄ‚îÄ reddit_crawler.py            # Reddit-Crawling-Logik
‚îÇ   ‚îú‚îÄ‚îÄ ticker_resolver.py           # Ticker ‚Üí Unternehmensnamen
‚îÇ   ‚îú‚îÄ‚îÄ summarize_ticker.py          # Zusammenfassungen erstellen
‚îÇ   ‚îú‚îÄ‚îÄ discord_utils.py             # Discord-Benachrichtigungen
‚îÇ   ‚îú‚îÄ‚îÄ ticker_utils.py              # Tickerlist-Verwaltung
‚îÇ   ‚îú‚îÄ‚îÄ __version__.py               # Version-Info
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îî‚îÄ‚îÄ test_discord_message.py  # Discord-Test-Tool
‚îú‚îÄ‚îÄ data/                             # Persistente Daten
‚îÇ   ‚îú‚îÄ‚îÄ input/                       # Ticker-Listen & Caches
‚îÇ   ‚îú‚îÄ‚îÄ output/                      # Crawl-Ergebnisse & Summaries
‚îÇ   ‚îî‚îÄ‚îÄ state/                       # Alert-States
‚îú‚îÄ‚îÄ logs/                             # Log-Dateien
‚îú‚îÄ‚îÄ config/                           # Konfiguration
‚îÇ   ‚îú‚îÄ‚îÄ .env                         # Deine Credentials (nicht in Git!)
‚îÇ   ‚îî‚îÄ‚îÄ .env.example                 # Vorlage
‚îú‚îÄ‚îÄ Dockerfile                        # Docker-Image
‚îú‚îÄ‚îÄ docker-compose.yml               # Docker Orchestration
‚îî‚îÄ‚îÄ requirements.txt                 # Python-Dependencies
```

## üí° Verwendung

---

## üì± Discord-Alarm-Format

Die Alerts sind kompakt und informativ gestaltet:

```
‚ö†Ô∏è WSB-ALARM ‚Äî Ungew√∂hnliche Aktivit√§t entdeckt
üíæ 260202-143022_crawler-ergebnis.pkl
‚è∞ 02.02.2026 14:30:22

ü•á GME - GameStop Corp. üö®
üî¢ Nennungen: 127 (Œî +85)
üíµ 42.50 USD (+2.30 USD, +5.73%) üìà [02.02.2026 14:28]
    | üåÖ Pre-Market: 41.80 USD | üåô After-Market: 42.90 USD
    | Trends: 1h ‚ñ≤ +1.2% ¬∑ 24h ‚ñ≤ +5.7% ¬∑ 7d ‚ñº -2.3%
    | https://finance.yahoo.com/quote/GME
üß† Kurs: 42.50 USD (Œî +2.30, +5.73%). Aktuelle Headlines: ...
üì∞ GameStop rallies on new NFT marketplace (Reuters) | https://...
üì∞ Analysts upgrade GME price target (Bloomberg) | https://...
---
```

### Alert-Bedingungen

Ein Alert wird ausgel√∂st, wenn:

- **Neue Ticker:** ‚â• 20 Nennungen (konfigurierbar: `ALERT_MIN_ABS`)
- **Bekannte Ticker:** 
  - Anstieg ‚â• 10 Nennungen (`ALERT_MIN_DELTA`)
  - **UND** ‚â• 200% des vorherigen Werts (`ALERT_RATIO`)
- Optional: Kursver√§nderung ‚â• 5% (`ALERT_MIN_PRICE_MOVE`)

Pro Crawl werden max. 3 Alerts gesendet (`ALERT_MAX_PER_RUN`) mit 4h Cooldown pro Ticker (`ALERT_COOLDOWN_H`)
cp config/.env.example config/.env
nano config/.env

---

## ÔøΩ Docker

### Pre-built Images

Das Projekt stellt automatisch gebaute Docker Images bereit:

| Tag | Beschreibung | Verwendung |
|-----|--------------|------------|
| `latest` | Neueste Version vom main branch | Empfohlen f√ºr Production |
| `v1.0.1` | Spezifische Release-Version | F√ºr Reproduzierbarkeit |
| `v1.0` | Minor-Version (automatisch) | Latest Patch einer Minor-Version |

```bash
# Latest Version
docker pull ghcr.io/fgrfn/reddit-wsb-crawler:latest
docker run --env-file config/.env \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/logs:/app/logs \
  ghcr.io/fgrfn/reddit-wsb-crawler:latest

# Spezifische Version f√ºr Reproduzierbarkeit
docker pull ghcr.io/fgrfn/reddit-wsb-crawler:v1.0.1
```

### Docker Compose (empfohlen)

Das Repo enth√§lt ein vollst√§ndiges `docker-compose.yml` mit allen Konfigurationen.

**1. Einmalig ausf√ºhren:**
```bash
# Mit lokalem Build
docker-compose up

# Mit Pre-built Image
docker-compose -f docker-compose.prod.yml up
```

**2. Mit Scheduler (regelm√§√üige Crawls):**
```bash
# F√ºhrt Crawler im Intervall aus (Standard: alle 60 Minuten)
docker-compose --profile scheduler up -d

# Logs anschauen
docker-compose logs -f wsb-crawler-scheduler

# Intervall anpassen (z.B. 15 Minuten)
CRAWL_INTERVAL=900 docker-compose --profile scheduler up -d
```

**3. Beispiel `docker-compose.prod.yml`:**
```yaml
version: '3.8'

services:
  wsb-crawler:
    image: ghcr.io/fgrfn/reddit-wsb-crawler:latest
    container_name: wsb-crawler
    restart: unless-stopped
    
    environment:
      - TZ=Europe/Berlin
    
    env_file:
      - config/.env
    
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./config/.env:/app/config/.env:ro
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

### Erforderliche Environment-Variablen

Erstelle `config/.env` mit folgenden Variablen:

**Pflicht-Felder:**
```bash
# Reddit API (https://www.reddit.com/prefs/apps)
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_client_secret_here
REDDIT_USER_AGENT=python:wsb-crawler:v1.0.0 (by /u/yourusername)

# NewsAPI (https://newsapi.org/register)
NEWSAPI_KEY=your_newsapi_key_here

# Discord Webhook
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/YOUR_WEBHOOK_URL
```

**Optional (mit Defaults):**
```bash
# Subreddits (komma-separiert)
SUBREDDITS=wallstreetbets,wallstreetbetsGER

# News-Einstellungen
NEWSAPI_LANG=en                # Sprache (en, de, fr, etc.)
NEWSAPI_WINDOW_HOURS=48        # Zeitfenster f√ºr News

# Discord Status-Updates
DISCORD_STATUS_UPDATE=true     # Silent Status (ohne @everyone)

# Alert-Schwellwerte
ALERT_MIN_ABS=20              # Min. Nennungen f√ºr neue Ticker
ALERT_MIN_DELTA=10            # Min. Anstieg f√ºr bekannte Ticker
ALERT_RATIO=2.0               # Min. Faktor (200% des Vorwerts)
ALERT_MIN_PRICE_MOVE=5.0      # Min. Kurs√§nderung in %
ALERT_MAX_PER_RUN=3           # Max. Alerts pro Crawl
ALERT_COOLDOWN_H=4            # Cooldown pro Ticker in Stunden

# Scheduler (nur f√ºr --profile scheduler)
CRAWL_INTERVAL=3600           # Intervall in Sekunden (60 Min.)
```

Vollst√§ndige Vorlage: [`config/.env.example`](config/.env.example)

Mehr Details: [DOCKER.md](DOCKER.md)

---

## ÔøΩüîÑ Versioning & Releases

Das Projekt nutzt [Semantic Versioning](https://semver.org/) mit **vollautomatischem Release-Management**.

### Automatischer Workflow

Bei jedem Push auf `main`:

```mermaid
graph LR
    A[Code Push] --> B{Version ge√§ndert?}
    B -->|Nein| C[Auto-Increment Patch]
    B -->|Ja| D[Release erstellen]
    C --> D
    D --> E[Docker bauen]
    E --> F[Publish zu GHCR]
```

1. **Version-Check**: Pr√ºft ob `version.txt` manuell ge√§ndert wurde
2. **Auto-Increment**: Erh√∂ht Patch-Version (1.0.0 ‚Üí 1.0.1) falls nicht
3. **Release**: Erstellt GitHub Release mit automatischem Changelog
4. **Docker**: Baut und pusht Image zu `ghcr.io/fgrfn/reddit-wsb-crawler`

### Version manuell erh√∂hen

F√ºr Features oder Breaking Changes:

```bash
# Feature-Release (1.0.0 ‚Üí 1.1.0)
echo "1.1.0" > version.txt
sed -i 's/__version__ = ".*"/__version__ = "1.1.0"/' src/__version__.py
sed -i 's/version=".*"/version="1.1.0"/' Dockerfile
git commit -am "feat: neue Feature-Beschreibung"
git push

# Breaking Change (1.1.0 ‚Üí 2.0.0)
echo "2.0.0" > version.txt
sed -i 's/__version__ = ".*"/__version__ = "2.0.0"/' src/__version__.py
sed -i 's/version=".*"/version="2.0.0"/' Dockerfile
git commit -am "feat!: breaking change Beschreibung"
git push
```

**Aktuelle Version:** 1.0.0 | [Alle Releases ‚Üí](https://github.com/fgrfn/reddit-wsb-crawler/releases)

---

## üìñ Dokumentation

| Dokument | Beschreibung |
|----------|--------------|
| [DOCKER.md](DOCKER.md) | Umfassende Docker-Anleitung mit Best Practices |
| [CHANGELOG.md](CHANGELOG.md) | Vollst√§ndige Versions-Historie |
| [SETUP_COMPLETE.md](SETUP_COMPLETE.md) | Detaillierte Setup-√úbersicht |
| [REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md) | Code-Refactoring Details |

---

## üõ†Ô∏è Development

### Lokales Setup

```bash
# Repository klonen
git clone https://github.com/fgrfn/reddit-wsb-crawler.git
cd reddit-wsb-crawler

# Virtual Environment
python -m venv venv
source venv/bin/activate

# Dependencies installieren
pip install -r requirements.txt

# Pre-commit hooks (optional)
pip install pre-commit
pre-commit install
```

### Mit Docker entwickeln

```bash
# Development Container starten
docker-compose -f docker-compose.dev.yml up -d

# In Container einloggen
docker exec -it wsb-crawler-dev bash

# Im Container arbeiten
python src/run_crawler_headless.py
```

### Tests

```bash
# Einzelne Komponenten testen
python src/scripts/test_discord_message.py

# Ticker-Aufl√∂sung testen
python src/build_ticker_name_cache.py

# Interaktive Suche
python src/check_ticker_mentions.py
```

---

## ü§ù Contributing

Beitr√§ge sind willkommen! So kannst du helfen:

### Pull Request erstellen

1. **Fork** das Repository
2. **Branch** erstellen: `git checkout -b feature/amazing-feature`
3. **Commit** mit [Conventional Commits](https://www.conventionalcommits.org/):
   - `feat:` f√ºr neue Features
   - `fix:` f√ºr Bugfixes
   - `docs:` f√ºr Dokumentation
   - `chore:` f√ºr Wartungsarbeiten
4. **Push**: `git push origin feature/amazing-feature`
5. **Pull Request** √∂ffnen

### Code-Qualit√§t

- Type Hints f√ºr alle Funktionen
- Docstrings f√ºr √∂ffentliche APIs
- Tests f√ºr neue Features
- Docker-Kompatibilit√§t sicherstellen

---

## üìä Monitoring & Logs

### Logs ansehen

```bash
# Docker
docker-compose logs -f
docker-compose logs --tail=50 wsb-crawler

# Lokal
tail -f logs/crawler.log
```

### Log-Dateien

- `logs/crawler.log` - Hauptlog
- `logs/resolver.log` - Ticker-Namensaufl√∂sung
- `logs/openai_costs.log` - API-Kosten (falls verwendet)
- `logs/archive/` - Archivierte Logs

### Debugging

```bash
# Fehlerhafte Pickle-Dateien pr√ºfen
ls -la data/output/pickle/

# Cache leeren
rm data/input/ticker_name_map.pkl

# Ticker-Listen neu laden
python -c "from src.ticker_utils import download_and_clean_tickerlist; download_and_clean_tickerlist()"
```

---

## üîí Security

### Best Practices

- **Nie** `.env`-Dateien committen!
- API-Keys in GitHub Secrets f√ºr CI/CD
- Docker Secrets f√ºr Production
- Regelm√§√üige Dependency-Updates

### Secrets Management

```bash
# F√ºr Production: Docker Secrets
echo "my_secret_key" | docker secret create reddit_client_secret -

# In docker-compose.yml
secrets:
  - reddit_client_secret
```

---

## üìù License

MIT License - siehe [LICENSE](LICENSE)

---

## üôè Credits

- **Reddit API**: [PRAW](https://praw.readthedocs.io/)
- **Kursdaten**: [yfinance](https://github.com/ranaroussi/yfinance)
- **News**: [NewsAPI](https://newsapi.org/)
- **Discord**: [Webhooks](https://discord.com/developers/docs/resources/webhook)

---

## üí¨ Support

- **Issues**: [GitHub Issues](https://github.com/fgrfn/reddit-wsb-crawler/issues)
- **Discussions**: [GitHub Discussions](https://github.com/fgrfn/reddit-wsb-crawler/discussions)
- **Discord**: [Server beitreten](#) _(falls vorhanden)_

---

<div align="center">

**Entwickelt mit ‚ù§Ô∏è f√ºr die WSB-Community**

‚≠ê Wenn dir dieses Projekt gef√§llt, gib uns einen Star!

[‚¨Ü Nach oben](#-wsb-crawler)

</div>

```bash
# Alle 30 Minuten
CRAWL_INTERVAL=1800 docker-compose --profile scheduler up -d

# Alle 2 Stunden
CRAWL_INTERVAL=7200 docker-compose --profile scheduler up -d
```

### Development Mode

```bash
# Live-Reload f√ºr Code-√Ñnderungen
docker-compose -f docker-compose.dev.yml up -d
docker exec -it wsb-crawler-dev bash

# Im Container
python src/run_crawler_headless.py
```

üìö **Mehr Details:** Siehe [DOCKER.md](DOCKER.md)

---

## ‚öôÔ∏è Konfiguration

### Basis-Setup
Lege `config/.env` im Repo‚ÄëRoot an (empfohlen). Minimales Beispiel:
```ini
# Reddit
REDDIT_CLIENT_ID=...
REDDIT_CLIENT_SECRET=...
REDDIT_USER_AGENT=python:wsb-crawler:v1.0 (by /u/you)
SUBREDDITS=wallstreetbets,wallstreetbetsGER

# NewsAPI (erforderlich f√ºr Headlines)
NEWSAPI_KEY=dein_newsapi_key
NEWSAPI_LANG=en
NEWSAPI_WINDOW_HOURS=48

# Discord
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/...

# Alert thresholds (Beispiele)
ALERT_MIN_ABS=20
ALERT_MIN_DELTA=10
ALERT_RATIO=2.0
ALERT_MIN_PRICE_MOVE=5.0
ALERT_MAX_PER_RUN=3
ALERT_COOLDOWN_H=4
```
Hinweis: `config/.env` (Repo‚ÄëRoot) wird bevorzugt; es gibt Fallbacks (`src/config/.env`, `./config/.env`).

## üê≥ Docker (empfohlen)

Schnellster Weg zum Starten:

```bash
# 1. Config erstellen
cp config/.env.example config/.env
# Bearbeite config/.env mit deinen API-Keys

# 2. Container starten
docker-compose up --build

# Oder mit Scheduler (st√ºndliche Crawls)
docker-compose --profile scheduler up -d
```

Siehe [DOCKER.md](DOCKER.md) f√ºr Details.

## Schnellstart / Befehle
- Voller Headless‚ÄëCrawl:
```bash
python src/run_crawler_headless.py
```
- Lokale Discord‚ÄëPreview (nutzt neueste Pickle):
```bash
python src/scripts/test_discord_message.py --use-real
```
- Preview + senden (ben√∂tigt g√ºltigen `DISCORD_WEBHOOK_URL`):
```bash
python src/scripts/test_discord_message.py --use-real --send
```

## Dateistruktur (wichtig)
- data/input/ ‚Äî Tickerlisten & Caches (`ticker_name_map.pkl`, `all_tickers.csv`)
- data/output/pickle/ ‚Äî Crawl‚ÄëPickles (z. B. `251030-000011_crawler-ergebnis.pkl`)
- data/output/summaries/ ‚Äî generierte Zusammenfassungen (.md)
- data/state/ ‚Äî persistenter Alert‚ÄëState (z. B. `alerts.json`)
- src/ ‚Äî Quellcode (crawler, summarizer, discord utils, scripts)

## Discord‚ÄëAlarm‚ÄëFormat
Kompakte Alert‚ÄëNachricht enth√§lt:
- Kopf (‚ö†Ô∏è WSB‚ÄëALARM) + Pickle‚ÄëName (üíæ ...)
- Zeitstempel
- Top‚ÄëTicker (bis 3) mit Nennungen, Œî, Kurszeile (inkl. Pre/After‚ÄëMarket), Trends (1h/24h/7d), Yahoo‚ÄëLink
- Kurz‚ÄëSummary + bis zu 3 News‚ÄëHeadlines (Titel | Quelle | URL)

Ziel: schnelle visuelle Erkennung von pl√∂tzlichen Nennungs‚Äë/Trend‚ÄëAnstiegen.

## Testen von News‚ÄëFetch
Beispiel: NewsAPI‚ÄëAbruf pr√ºfen:
```bash
python - <<'PY'
import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd()/'src'))
from summarize_ticker import get_yf_news
import json
print(json.dumps(get_yf_news("BYND"), ensure_ascii=False, indent=2))
PY
```

## Alert‚ÄëLogik (Kurz)
Empfohlene, konfigurierte Schwellen (ENV):
- ALERT_MIN_ABS, ALERT_MIN_DELTA, ALERT_RATIO, ALERT_MIN_PRICE_MOVE, ALERT_MAX_PER_RUN, ALERT_COOLDOWN_H  
Vorschlag: Score‚Äëbasierter Filter + per‚ÄëTicker Cooldown; Ergebnisse persistent in `data/state/alerts.json`.

## Weiteres / Anpassungen
- `src/scripts/test_discord_message.py` erzeugt eine Vorschau‚ÄëNachricht; mit `--use-real` nutzt es die letzte Crawl‚ÄëPickle.

## üîÑ Updates & Versioning

Das Projekt nutzt [Semantic Versioning](https://semver.org/). Bei jedem Push auf `main` wird automatisch:
1. Die Version in `version.txt` um 0.0.1 erh√∂ht
2. Ein GitHub Release mit Changelog erstellt
3. Ein Docker-Image gebaut und in die GitHub Container Registry gepusht

**Aktuelle Version:** 1.0.0

Siehe [CHANGELOG.md](CHANGELOG.md) f√ºr Details zu allen Versionen.

## üìñ Weitere Dokumentation

- [DOCKER.md](DOCKER.md) - Docker Setup und Best Practices
- [REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md) - Code-Refactoring Details
- [CHANGELOG.md](CHANGELOG.md) - Versions-Historie

## ü§ù Contributing

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/amazing-feature`)
3. Commit deine √Ñnderungen (`git commit -m 'feat: add amazing feature'`)
4. Push zum Branch (`git push origin feature/amazing-feature`)
5. √ñffne einen Pull Request

## üìù License

MIT License - siehe [LICENSE](LICENSE) f√ºr Details.
- `src/summarize_ticker.py` erzeugt strukturierte Summaries: `{"summary": "...", "news": [...]}`.
- √Ñnderbare Parameter via ENV (ALERT_*, NEWSAPI_*).

## Troubleshooting
- Keine News angezeigt:
  - Pr√ºfe `NEWSAPI_KEY` in `config/.env` und Rate‚ÄëLimits von NewsAPI.
  - Stelle `NEWSAPI_WINDOW_HOURS` ggf. auf ein gr√∂√üeres Fenster.
- yfinance Errors (z. B. ung√ºltige period):
  - Achte auf g√ºltige `period`/`interval`-Kombinationen (`1d`, `5d`, `1mo`, ...).
- Falsche/alte Version importiert:
```bash
find . -name "__pycache__" -exec rm -rf {} +
python - <<'PY'
import inspect, sys
sys.path.insert(0, "src")
import discord_utils
print(discord_utils.__file__)
PY
```
- Logs pr√ºfen:
  - `logs/` (falls vorhanden) oder stdout der service/cron Unit.
- Alerts werden zu h√§ufig gesendet:
  - Erh√∂he `ALERT_COOLDOWN_H`, `ALERT_MIN_ABS`, `ALERT_MIN_DELTA` oder senke `ALERT_MAX_PER_RUN`.

## Lizenz
MIT

