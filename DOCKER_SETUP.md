# üê≥ Docker-Setup auf Unraid

## Behobene Probleme

Die folgenden Fehler wurden behoben:

### ‚úÖ 1. Fehlende NYSE- und DE/EU-Ticker-Listen
**Problem:** Warnungen "Keine NYSE-Liste gefunden" und "Keine DE/EU-Liste gefunden"

**L√∂sung:** Die Listen sind jetzt optional. Der Crawler funktioniert auch nur mit NASDAQ-Daten.

**Optional - Vollst√§ndige Ticker-Listen hinzuf√ºgen:**

Wenn du auch NYSE und europ√§ische Aktien tracken m√∂chtest:

1. **NYSE-Liste herunterladen:**
   ```bash
   # In deinem Unraid Docker data/input Verzeichnis
   wget https://datahub.io/core/nyse-other-listings/r/nyse-listed.csv -O nyse-listed-symbols.csv
   ```

2. **DE/EU-Liste (optional):**
   - Datei von [Deutsche B√∂rse](https://www.deutsche-boerse.com/dbg-de/ueber-uns/services/know-your-market/listen-statistiken) herunterladen
   - Als `de-listed-symbols.xlsx` im `data/input/` Verzeichnis speichern

### ‚úÖ 2. systemctl-Fehler in Docker
**Problem:** `[Errno 2] No such file or directory: 'systemctl'`

**L√∂sung:** Der Code erkennt jetzt automatisch Docker-Umgebungen und zeigt "unbekannt (Docker)" f√ºr den n√§chsten Crawl-Zeitpunkt.

### ‚úÖ 3. Discord-Benachrichtigungs-Fehler
**Problem:** `ERROR: Fehler bei der Discord-Benachrichtigung: 'Kurs'`

**L√∂sung:** Kursdaten-Zugriff wurde abgesichert. Kurse werden jetzt separat nach der Aggregation geladen.

## Verzeichnisstruktur f√ºr Docker

Stelle sicher, dass folgende Verzeichnisse gemappt sind:

```
/mnt/user/appdata/reddit-wsb-crawler/
‚îú‚îÄ‚îÄ config/              # Konfigurationsdateien
‚îÇ   ‚îî‚îÄ‚îÄ .env            # API-Keys und Einstellungen
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/          # Ticker-Listen (optional: NYSE, DE/EU)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îú‚îÄ‚îÄ output/         # Crawl-Ergebnisse
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pickle/     # Pickle-Dateien mit Treffer-Daten
‚îÇ   ‚îî‚îÄ‚îÄ state/          # Status-Daten
‚îî‚îÄ‚îÄ logs/               # Log-Dateien
```

## Docker-Compose Beispiel f√ºr Unraid

```yaml
version: '3.8'

services:
  reddit-wsb-crawler:
    image: dein-image:latest
    container_name: reddit-wsb-crawler
    restart: unless-stopped
    environment:
      - TZ=Europe/Berlin
    volumes:
      - /mnt/user/appdata/reddit-wsb-crawler/config:/workspaces/reddit-wsb-crawler/config
      - /mnt/user/appdata/reddit-wsb-crawler/data:/workspaces/reddit-wsb-crawler/data
      - /mnt/user/appdata/reddit-wsb-crawler/logs:/workspaces/reddit-wsb-crawler/logs
```

## Umgebungsvariablen (.env)

Die `.env`-Datei im `config/` Verzeichnis sollte folgende Variablen enthalten:

```bash
# Reddit API
REDDIT_CLIENT_ID=dein_client_id
REDDIT_CLIENT_SECRET=dein_secret
REDDIT_USER_AGENT="WSB Crawler by u/deinusername"

# Discord Webhook
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/...

# OpenAI (f√ºr Zusammenfassungen)
OPENAI_API_KEY=sk-...

# Alpha Vantage (optional, f√ºr Ticker-Aufl√∂sung)
ALPHA_VANTAGE_API_KEY=dein_key

# Discord User IDs (optional, f√ºr Benachrichtigungen)
ADMIN_DISCORD_USER_IDS=123456789,987654321
```

## Erste Schritte

1. **Verzeichnisse erstellen:**
   ```bash
   mkdir -p /mnt/user/appdata/reddit-wsb-crawler/{config,data/{input,output,state},logs}
   ```

2. **`.env`-Datei erstellen:**
   ```bash
   nano /mnt/user/appdata/reddit-wsb-crawler/config/.env
   # F√ºge deine API-Keys ein
   ```

3. **Container starten:**
   - In Unraid: Docker-Tab ‚Üí Add Container
   - Oder via Docker Compose

4. **Logs √ºberpr√ºfen:**
   ```bash
   docker logs -f reddit-wsb-crawler
   ```

## Hinweise

- **NASDAQ-Liste:** Wird automatisch online geladen (keine Aktion n√∂tig)
- **NYSE/DE-EU-Listen:** Optional - nur wenn du diese M√§rkte auch tracken m√∂chtest
- **systemd-Timer:** Funktioniert nicht in Docker - nutze stattdessen Cron oder Docker-eigene Scheduler
- **Speicherplatz:** Stelle sicher, dass genug Platz f√ºr Logs und Pickle-Dateien vorhanden ist

## Troubleshooting

### Logs bleiben leer
**Problem:** Log-Dateien werden erstellt, bleiben aber leer (`crawler.log`, `openai_costs_crawl.log`)

**L√∂sung:**
1. **Berechtigungen pr√ºfen:**
   ```bash
   # Auf Unraid Host
   ls -la /mnt/user/appdata/reddit-wsb-crawler/logs/
   # Sollte vom Container-User beschreibbar sein
   
   # Falls Probleme: Berechtigungen anpassen
   chmod -R 777 /mnt/user/appdata/reddit-wsb-crawler/logs/
   ```

2. **Test-Script ausf√ºhren:**
   ```bash
   docker exec -it reddit-wsb-crawler python test_logging.py
   ```
   
3. **Container-Logs pr√ºfen:**
   ```bash
   docker logs reddit-wsb-crawler
   # Logs sollten auch in der Console erscheinen
   ```

4. **Volume-Mapping pr√ºfen:**
   ```bash
   docker inspect reddit-wsb-crawler | grep -A 10 Mounts
   # Sollte /app/logs -> /mnt/user/appdata/reddit-wsb-crawler/logs zeigen
   ```

**Hinweis:** Die neueste Version hat Auto-Flush f√ºr alle Logs aktiviert (`PYTHONUNBUFFERED=1` + Custom FlushFileHandler).

### Keine Crawl-Ergebnisse
```bash
# Pr√ºfe, ob Reddit API-Keys korrekt sind
docker exec -it reddit-wsb-crawler cat config/.env | grep REDDIT
```

### Discord-Benachrichtigung funktioniert nicht
```bash
# Teste Webhook manuell
curl -X POST -H "Content-Type: application/json" \
  -d '{"content":"Test"}' \
  https://discord.com/api/webhooks/DEINE_WEBHOOK_URL
```

### Container startet nicht
```bash
# Pr√ºfe Logs
docker logs reddit-wsb-crawler

# Pr√ºfe Berechtigungen
ls -la /mnt/user/appdata/reddit-wsb-crawler/
```
