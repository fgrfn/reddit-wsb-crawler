import os
import shutil
import hashlib
from datetime import datetime
from pathlib import Path
import zipfile

def archive_log(log_path: Path, archive_dir: Path, zip_old_logs: bool = False, keep_last: int = None):
    if not log_path.exists():
        print("‚ö†Ô∏è Kein Logfile zum Archivieren.")
        return

    archive_dir.mkdir(parents=True, exist_ok=True)

    # üóìÔ∏è Zeitstempel f√ºr eindeutigen Namen
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    base_name = log_path.stem
    archive_file = archive_dir / f"{base_name}_{timestamp}.log"

    # üß† Deduplizieren: Falls identischer Hash schon im Archiv, abbrechen
    with open(log_path, "rb") as f:
        log_data = f.read()
        current_hash = hashlib.sha256(log_data).hexdigest()

    for existing_log in archive_dir.glob(f"{base_name}_*.log"):
        with open(existing_log, "rb") as f:
            if hashlib.sha256(f.read()).hexdigest() == current_hash:
                print("‚ö†Ô∏è Identisches Logfile existiert bereits im Archiv. Kein Duplikat gespeichert.")
                return

    shutil.move(str(log_path), str(archive_file))
    print(f"‚úÖ Logfile archiviert unter: {archive_file}")

    # üéÅ Optional: Alte Logs zippen, um Platz zu sparen
    if zip_old_logs:
        for old_log in archive_dir.glob(f"{base_name}_*.log"):
            zip_path = old_log.with_suffix(".zip")
            with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                zf.write(old_log, arcname=old_log.name)
            old_log.unlink()
            print(f"üì¶ Archiv komprimiert: {zip_path.name}")

    # üßπ √Ñltere Logs l√∂schen, wenn Limit gesetzt ist
    if keep_last is not None:
        archived_logs = sorted(
            archive_dir.glob(f"{base_name}_*.log"),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )
        for old_file in archived_logs[keep_last:]:
            old_file.unlink()
            print(f"üßπ Alte Log-Datei gel√∂scht: {old_file.name}")
